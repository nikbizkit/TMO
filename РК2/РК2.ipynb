{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Value\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"dataset/sentiment labelled sentences/amazon_cells_labelled.txt\", \n",
    "                   delimiter='\\t', header=None, names=['Text', 'Value'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Общий словарь для обучения моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
       " 'Good case, Excellent value.',\n",
       " 'Great for the jawbone.',\n",
       " 'Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!',\n",
       " 'The mic is great.',\n",
       " 'I have to jiggle the plug to get it to line up right to get decent volume.',\n",
       " 'If you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one.',\n",
       " 'If you are Razr owner...you must have this!',\n",
       " 'Needless to say, I wasted my money.',\n",
       " 'What a waste of money and time!.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = data.Text.tolist()\n",
    "vocab_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x1847 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9130 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabVect = CountVectorizer()\n",
    "vocabVect.fit_transform(vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Количество признаков = 1847"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1847"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabVect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusVocab = vocabVect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Признак и его индекс в словаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so=1491\n",
      "there=1609\n",
      "is=854\n",
      "no=1074\n",
      "way=1766\n",
      "for=653\n",
      "me=993\n",
      "to=1640\n",
      "plug=1212\n",
      "it=857\n"
     ]
    }
   ],
   "source": [
    "for i in list(corpusVocab)[:10]:\n",
    "    print('{}={}'.format(i, corpusVocab[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x1847 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9130 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = vocabVect.transform(vocab_list)\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1000 строк - 1000 предложений в документе\n",
    "### 1847 столбцов - 1847 уникальных значений в документе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x15088 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 25421 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncv = CountVectorizer(ngram_range=(1, 3))\n",
    "ngram_features = ncv.fit_transform(vocab_list)\n",
    "ngram_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able to',\n",
       " 'able to do',\n",
       " 'able to roam',\n",
       " 'able to use',\n",
       " 'abound',\n",
       " 'about',\n",
       " 'about 10',\n",
       " 'about 10 of',\n",
       " 'about 18',\n",
       " 'about 18 months',\n",
       " 'about inches',\n",
       " 'about inches above',\n",
       " 'about it',\n",
       " 'about it is',\n",
       " 'about the',\n",
       " 'about the consumer',\n",
       " 'about this',\n",
       " 'about this headset',\n",
       " 'about this phone',\n",
       " 'about this product']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncv.get_feature_names()[100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизация TfidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x15088 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 25421 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfv = TfidfVectorizer(ngram_range=(1,3))\n",
    "tfidf_ngram_features = tfidfv.fit_transform(vocab_list)\n",
    "tfidf_ngram_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_ngram_features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12296719867492838,\n",
       " 0.15534944608172185,\n",
       " 0.15534944608172185,\n",
       " 0.06830400100424172,\n",
       " 0.1255030252282181,\n",
       " 0.15534944608172185,\n",
       " 0.1283779082640305,\n",
       " 0.15534944608172185,\n",
       " 0.15534944608172185,\n",
       " 0.13562203495268255]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Непустые значения нулевой строки\n",
    "[i for i in tfidf_ngram_features.todense()[0].getA1() if i>0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VectorizeAndClassify(vectorizers_list, classifiers_list):\n",
    "    for v in vectorizers_list:\n",
    "        for c in classifiers_list:\n",
    "            pipeline1 = Pipeline([(\"vectorizer\", v), (\"classifier\", c)])\n",
    "            score = cross_val_score(pipeline1, data['Text'], data['Value'], scoring='accuracy', cv=3).mean()\n",
    "            print('Векторизация - {}'.format(v))\n",
    "            print('Модель для классификации - {}'.format(c))\n",
    "            print('Accuracy = {}'.format(score))\n",
    "            print('===========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, NuSVC, LinearSVC, OneClassSVM, SVR, NuSVR, LinearSVR\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None,\n",
      "                vocabulary={'10': 0, '100': 1, '11': 2, '12': 3, '13': 4,\n",
      "                            '15': 5, '15g': 6, '18': 7, '20': 8, '2000': 9,\n",
      "                            '2005': 10, '2160': 11, '24': 12, '2mp': 13,\n",
      "                            '325': 14, '350': 15, '375': 16, '3o': 17, '42': 18,\n",
      "                            '44': 19, '45': 20, '4s': 21, '50': 22, '5020': 23,\n",
      "                            '510': 24, '5320': 25, '680': 26, '700w': 27,\n",
      "                            '8125': 28, '8525': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy = 0.8069896243548937\n",
      "===========================\n",
      "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None,\n",
      "                vocabulary={'10': 0, '100': 1, '11': 2, '12': 3, '13': 4,\n",
      "                            '15': 5, '15g': 6, '18': 7, '20': 8, '2000': 9,\n",
      "                            '2005': 10, '2160': 11, '24': 12, '2mp': 13,\n",
      "                            '325': 14, '350': 15, '375': 16, '3o': 17, '42': 18,\n",
      "                            '44': 19, '45': 20, '4s': 21, '50': 22, '5020': 23,\n",
      "                            '510': 24, '5320': 25, '680': 26, '700w': 27,\n",
      "                            '8125': 28, '8525': 29, ...})\n",
      "Модель для классификации - LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n",
      "Accuracy = 0.8249956543369716\n",
      "===========================\n",
      "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None,\n",
      "                vocabulary={'10': 0, '100': 1, '11': 2, '12': 3, '13': 4,\n",
      "                            '15': 5, '15g': 6, '18': 7, '20': 8, '2000': 9,\n",
      "                            '2005': 10, '2160': 11, '24': 12, '2mp': 13,\n",
      "                            '325': 14, '350': 15, '375': 16, '3o': 17, '42': 18,\n",
      "                            '44': 19, '45': 20, '4s': 21, '50': 22, '5020': 23,\n",
      "                            '510': 24, '5320': 25, '680': 26, '700w': 27,\n",
      "                            '8125': 28, '8525': 29, ...})\n",
      "Модель для классификации - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "Accuracy = 0.6619733505961051\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True,\n",
      "                vocabulary={'10': 0, '100': 1, '11': 2, '12': 3, '13': 4,\n",
      "                            '15': 5, '15g': 6, '18': 7, '20': 8, '2000': 9,\n",
      "                            '2005': 10, '2160': 11, '24': 12, '2mp': 13,\n",
      "                            '325': 14, '350': 15, '375': 16, '3o': 17, '42': 18,\n",
      "                            '44': 19, '45': 20, '4s': 21, '50': 22, '5020': 23,\n",
      "                            '510': 24, '5320': 25, '680': 26, '700w': 27,\n",
      "                            '8125': 28, '8525': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy = 0.8109936283588978\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True,\n",
      "                vocabulary={'10': 0, '100': 1, '11': 2, '12': 3, '13': 4,\n",
      "                            '15': 5, '15g': 6, '18': 7, '20': 8, '2000': 9,\n",
      "                            '2005': 10, '2160': 11, '24': 12, '2mp': 13,\n",
      "                            '325': 14, '350': 15, '375': 16, '3o': 17, '42': 18,\n",
      "                            '44': 19, '45': 20, '4s': 21, '50': 22, '5020': 23,\n",
      "                            '510': 24, '5320': 25, '680': 26, '700w': 27,\n",
      "                            '8125': 28, '8525': 29, ...})\n",
      "Модель для классификации - LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n",
      "Accuracy = 0.8109816403229577\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True,\n",
      "                vocabulary={'10': 0, '100': 1, '11': 2, '12': 3, '13': 4,\n",
      "                            '15': 5, '15g': 6, '18': 7, '20': 8, '2000': 9,\n",
      "                            '2005': 10, '2160': 11, '24': 12, '2mp': 13,\n",
      "                            '325': 14, '350': 15, '375': 16, '3o': 17, '42': 18,\n",
      "                            '44': 19, '45': 20, '4s': 21, '50': 22, '5020': 23,\n",
      "                            '510': 24, '5320': 25, '680': 26, '700w': 27,\n",
      "                            '8125': 28, '8525': 29, ...})\n",
      "Модель для классификации - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "Accuracy = 0.7709895524266782\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "vectorizers_list = [CountVectorizer(vocabulary = corpusVocab), TfidfVectorizer(vocabulary = corpusVocab)]\n",
    "classifiers_list = [LogisticRegression(C=3.0), LinearSVC(), KNeighborsClassifier()]\n",
    "VectorizeAndClassify(vectorizers_list, classifiers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделение выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['Text'], data['Value'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "def accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Вычисление метрики accuracy для каждого класса\n",
    "    y_true - истинные значения классов\n",
    "    y_pred - предсказанные значения классов\n",
    "    Возвращает словарь: ключ - метка класса, \n",
    "    значение - Accuracy для данного класса\n",
    "    \"\"\"\n",
    "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
    "    d = {'t': y_true, 'p': y_pred}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # Метки классов\n",
    "    classes = np.unique(y_true)\n",
    "    # Результирующий словарь\n",
    "    res = dict()\n",
    "    # Перебор меток классов\n",
    "    for c in classes:\n",
    "        # отфильтруем данные, которые соответствуют \n",
    "        # текущей метке класса в истинных значениях\n",
    "        temp_data_flt = df[df['t']==c]\n",
    "        # расчет accuracy для заданной метки класса\n",
    "        temp_acc = accuracy_score(\n",
    "            temp_data_flt['t'].values, \n",
    "            temp_data_flt['p'].values)\n",
    "        # сохранение результата в словарь\n",
    "        res[c] = temp_acc\n",
    "    return res\n",
    "\n",
    "def print_accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Вывод метрики accuracy для каждого класса\n",
    "    \"\"\"\n",
    "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
    "    if len(accs)>0:\n",
    "        print('Метка \\t Accuracy')\n",
    "    for i in accs:\n",
    "        print('{} \\t {}'.format(i, accs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(v, c):\n",
    "    model = Pipeline(\n",
    "        [(\"vectorizer\", v), \n",
    "         (\"classifier\", c)])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print_accuracy_score_for_classes(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.8099173553719008\n",
      "1 \t 0.8023255813953488\n",
      "============================\n",
      "Метка \t Accuracy\n",
      "0 \t 0.7975206611570248\n",
      "1 \t 0.7984496124031008\n",
      "============================\n",
      "Метка \t Accuracy\n",
      "0 \t 0.7727272727272727\n",
      "1 \t 0.6162790697674418\n",
      "============================\n",
      "Метка \t Accuracy\n",
      "0 \t 0.7975206611570248\n",
      "1 \t 0.8178294573643411\n",
      "============================\n",
      "Метка \t Accuracy\n",
      "0 \t 0.768595041322314\n",
      "1 \t 0.6162790697674418\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "types = [[TfidfVectorizer(), LogisticRegression(C=5.0)], \n",
    "        [TfidfVectorizer(ngram_range=(1,3)), LogisticRegression(C=5.0)],\n",
    "        [TfidfVectorizer(ngram_range=(2,3)), LogisticRegression(C=5.0)],\n",
    "        [TfidfVectorizer(ngram_range=(1,4)), LogisticRegression(C=5.0)],\n",
    "        [TfidfVectorizer(ngram_range=(2,4)), LogisticRegression(C=5.0)]]\n",
    "for type_ in types:\n",
    "    sentiment(*type_)\n",
    "    print(\"============================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
